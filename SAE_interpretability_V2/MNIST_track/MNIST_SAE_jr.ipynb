{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0: Clean reinstall Torch stack for CUDA 12.1 (Colab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Strategy: Install transformer_lens first (which upgrades torch to 2.6+),\n",
        "# then reinstall matching torchvision/torchaudio for that torch version\n",
        "\n",
        "!pip uninstall -y torch torchvision torchaudio pytorch-lightning numpy\n",
        "\n",
        "# Install transformer_lens first (will pull torch 2.6+)\n",
        "!pip install transformer_lens\n",
        "\n",
        "# Now reinstall matching torch/torchvision/torchaudio together\n",
        "!pip install --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install remaining dependencies\n",
        "!pip install \"pytorch-lightning<3\" \"numpy>=1.26,<2\"\n",
        "!pip install git+https://github.com/neelnanda-io/Tracr\n",
        "\n",
        "# Restart warning\n",
        "print(\"\\n⚠️  IMPORTANT: After installation completes, you MUST restart the runtime!\")\n",
        "print(\"    Go to: Runtime → Restart runtime\")\n",
        "print(\"    Then run the cells starting from Cell 1 (skip Cell 0).\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1: Imports & constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "SEED = 42\n",
        "pl.seed_everything(SEED, workers=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# FFN hyperparameters\n",
        "FFN_TYPE = \"ReLU\"\n",
        "HIDDEN_DIM = 128\n",
        "LR_FFN = 1e-3\n",
        "EPOCHS_FFN = 5\n",
        "BATCH_FFN = 8\n",
        "\n",
        "# SAE hyperparameters\n",
        "SAE_LATENTS      = 32\n",
        "TAU              = 0.1\n",
        "INIT_THETA       = 0.5\n",
        "LR_SAE_FINAL     = 1e-3\n",
        "EPOCHS_SAE_FINAL = 5\n",
        "BATCH_SAE        = 8\n",
        "NORMALIZE_Y      = True\n",
        "TARGET_ACTIVES   = [1, 2, 4]\n",
        "\n",
        "OUT_DIR = \"artifacts_reversal_sae\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def load_mnist(batch_size=BATCH_FFN):\n",
        "    tfm = transforms.Compose([transforms.ToTensor()])\n",
        "    ds_train_full = datasets.MNIST(root=\".\", train=True,  download=True, transform=tfm)\n",
        "    ds_test       = datasets.MNIST(root=\".\", train=False, download=True, transform=tfm)\n",
        "\n",
        "    train_len  = int(0.8 * len(ds_train_full))\n",
        "    val_len    = len(ds_train_full) - train_len\n",
        "    ds_train, ds_val = random_split(ds_train_full, [train_len, val_len], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "    pin = torch.cuda.is_available()\n",
        "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=pin)\n",
        "    dl_val   = DataLoader(ds_val,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=pin)\n",
        "    dl_test  = DataLoader(ds_test,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=pin)\n",
        "    return dl_train, dl_val, dl_test\n",
        "\n",
        "dl_train, dl_val, dl_test = load_mnist()\n",
        "\n",
        "\n",
        "class FFN_GeGLU(nn.Module):\n",
        "    def __init__(self, d_i, d_h, d_o):\n",
        "        super().__init__()\n",
        "        self.W_in_ih   = nn.Parameter(torch.randn(d_i, d_h) * 0.02)\n",
        "        self.W_gate_ih = nn.Parameter(torch.randn(d_i, d_h) * 0.02)\n",
        "        self.W_out_ho  = nn.Parameter(torch.randn(d_h, d_o) * 0.02)\n",
        "    def forward(self, x_bi):  # x_bi: [batch, input]\n",
        "        x_proj_bh = torch.einsum('bi,ih->bh', x_bi, self.W_in_ih)\n",
        "        gate_bh   = F.gelu(torch.einsum('bi,ih->bh', x_bi, self.W_gate_ih))\n",
        "        h_bh = x_proj_bh * gate_bh\n",
        "        y_bo = torch.einsum('bh,ho->bo', h_bh, self.W_out_ho)  # logits\n",
        "        return y_bo\n",
        "\n",
        "class FFN_ReLU(nn.Module):\n",
        "    def __init__(self, d_i, d_h, d_o):\n",
        "        super().__init__()\n",
        "        self.W_in_ih  = nn.Parameter(torch.randn(d_i, d_h) * 0.02)\n",
        "        self.W_out_ho = nn.Parameter(torch.randn(d_h, d_o) * 0.02)\n",
        "    def forward(self, x_bi):\n",
        "        z_bh = torch.einsum('bi,ih->bh', x_bi, self.W_in_ih)\n",
        "        h_bh = F.relu(z_bh)\n",
        "        y_bo = torch.einsum('bh,ho->bo', h_bh, self.W_out_ho)  # logits\n",
        "        return y_bo\n",
        "\n",
        "class MNIST_FFN(pl.LightningModule):\n",
        "    def __init__(self, ffn_type=\"ReLU\", d_h=128, lr=1e-3):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        d_i, d_o = 28*28, 10\n",
        "        if ffn_type == \"GeGLU\":\n",
        "            self.ffn = FFN_GeGLU(d_i, d_h, d_o)\n",
        "        elif ffn_type == \"ReLU\":\n",
        "            self.ffn = FFN_ReLU(d_i, d_h, d_o)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid ffn_type\")\n",
        "    def forward(self, x_bchw):\n",
        "        x_bi = x_bchw.view(x_bchw.size(0), -1)\n",
        "        y_bo = self.ffn(x_bi)\n",
        "        return y_bo\n",
        "    def training_step(self, batch, _):\n",
        "        x_bchw, y_gt = batch\n",
        "        y_bo = self(x_bchw)\n",
        "        return F.cross_entropy(y_bo, y_gt)\n",
        "    def validation_step(self, batch, _):\n",
        "        x_bchw, y_gt = batch\n",
        "        y_bo = self(x_bchw)\n",
        "        acc = (y_bo.argmax(dim=1) == y_gt).float().mean()\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "def _trainer(max_epochs):\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    has_bf16 = getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()\n",
        "    precision = \"bf16-mixed\" if (use_gpu and has_bf16) else (16 if use_gpu else 32)\n",
        "    return pl.Trainer(\n",
        "        max_epochs=max_epochs,\n",
        "        accelerator=\"gpu\" if use_gpu else \"cpu\",\n",
        "        devices=1,\n",
        "        precision=precision,\n",
        "        logger=False,\n",
        "        enable_checkpointing=False,\n",
        "    )\n",
        "\n",
        "model = MNIST_FFN(FFN_TYPE, HIDDEN_DIM, LR_FFN)\n",
        "_tr = _trainer(EPOCHS_FFN)\n",
        "_tr.fit(model, dl_train, dl_val)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def baseline_accuracy(model, loader):\n",
        "    model.eval().to(device)\n",
        "    correct=total=0\n",
        "    for x_bchw, y_gt in loader:\n",
        "        y_bo = model(x_bchw.to(device))\n",
        "        pred = y_bo.argmax(dim=1)\n",
        "        correct += (pred == y_gt.to(device)).sum().item()\n",
        "        total   += y_gt.numel()\n",
        "    return correct / max(total,1)\n",
        "\n",
        "base_test_acc = baseline_accuracy(model, dl_test)\n",
        "print(f\"Baseline test accuracy: {base_test_acc:.4f}\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_logits_buffers(model, loader):\n",
        "    model.eval().to(device)\n",
        "    feats = []\n",
        "    for x_bchw, _ in loader:\n",
        "        x_bi = x_bchw.to(device).view(x_bchw.size(0), -1)\n",
        "        y_bo = model.ffn(x_bi)\n",
        "        feats.append(y_bo.cpu())\n",
        "    return torch.cat(feats, dim=0)\n",
        "\n",
        "Ytr_bo = collect_logits_buffers(model, dl_train)\n",
        "Yva_bo = collect_logits_buffers(model, dl_val)\n",
        "Yte_bo = collect_logits_buffers(model, dl_test)\n",
        "print(\"Buffers (logits):\", Ytr_bo.shape, Yva_bo.shape, Yte_bo.shape)\n",
        "\n",
        "if NORMALIZE_Y:\n",
        "    mu_bo  = Ytr_bo.mean(0, keepdim=True)\n",
        "    std_bo = Ytr_bo.std(0, keepdim=True).clamp_min(1e-6)\n",
        "    Ytr_n = (Ytr_bo - mu_bo) / std_bo\n",
        "    Yva_n = (Yva_bo - mu_bo) / std_bo\n",
        "    Yte_n = (Yte_bo - mu_bo) / std_bo\n",
        "else:\n",
        "    mu_bo, std_bo = 0.0, 1.0\n",
        "    Ytr_n, Yva_n, Yte_n = Ytr_bo, Yva_bo, Yte_bo\n",
        "\n",
        "\n",
        "class SAE_JumpReLU(pl.LightningModule):\n",
        "    def __init__(self, d_in, d_latents=32, lambda_l0=1e-2, lr=1e-3, init_theta=0.5, tau=0.1):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.enc = nn.Linear(d_in, d_latents, bias=True)\n",
        "        self.theta_h = nn.Parameter(torch.full((d_latents,), float(init_theta)))\n",
        "        self.tau   = tau\n",
        "        self.dec = nn.Linear(d_latents, d_in, bias=True)\n",
        "    def forward(self, y_bO):\n",
        "        u_bh = self.enc(y_bO)\n",
        "        soft_bh = torch.sigmoid((u_bh - self.theta_h) / self.tau)\n",
        "        hard_bh = (u_bh > self.theta_h).float()\n",
        "        gate_bh = (hard_bh - soft_bh).detach() + soft_bh\n",
        "        f_bh = u_bh * gate_bh\n",
        "        y_hat_bO = self.dec(f_bh)\n",
        "        return y_hat_bO, f_bh, u_bh, soft_bh, hard_bh\n",
        "    def _step(self, batch):\n",
        "        (y_bO,) = batch\n",
        "        y_hat_bO, f_bh, u_bh, soft_bh, hard_bh = self(y_bO)\n",
        "        recon = F.mse_loss(y_hat_bO, y_bO, reduction=\"mean\")\n",
        "        l0_soft = soft_bh.sum(dim=1).mean()\n",
        "        loss = recon + self.hparams.lambda_l0 * l0_soft\n",
        "        l0_hard = hard_bh.sum(dim=1).float().mean().detach()\n",
        "        return loss, recon.detach(), l0_soft.detach(), l0_hard\n",
        "    def training_step(self, batch, _):\n",
        "        loss, recon, l0_soft, l0_hard = self._step(batch)\n",
        "        self.log_dict({\"train/recon_mse\": recon, \"train/l0_soft\": l0_soft, \"train/l0_hard\": l0_hard}, prog_bar=True)\n",
        "        return loss\n",
        "    def validation_step(self, batch, _):\n",
        "        loss, recon, l0_soft, l0_hard = self._step(batch)\n",
        "        self.log_dict({\"val/recon_mse\": recon, \"val/l0_soft\": l0_soft, \"val/l0_hard\": l0_hard}, prog_bar=True)\n",
        "    def on_after_backward(self):\n",
        "        with torch.no_grad():\n",
        "            W_hO = self.dec.weight.data\n",
        "            norms = W_hO.norm(dim=0, keepdim=True).clamp_min(1e-8)\n",
        "            self.dec.weight.data = W_hO / norms\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "def tloader(t, bs=BATCH_SAE, shuffle=True):\n",
        "    pin = torch.cuda.is_available()\n",
        "    return DataLoader(TensorDataset(t), batch_size=bs, shuffle=shuffle, num_workers=2, pin_memory=pin)\n",
        "\n",
        "@torch.no_grad()\n",
        "def gate_counts(sae, Y_bO, batch=1024):\n",
        "    sae.eval().to(device)\n",
        "    tots = []\n",
        "    for i in range(0, Y_bO.size(0), batch):\n",
        "        u_bh = sae.enc(Y_bO[i:i+batch].to(device))\n",
        "        hard_bh = (u_bh > sae.theta_h).float()\n",
        "        tots.append(hard_bh.sum(dim=1).cpu())\n",
        "    return float(torch.cat(tots).mean())\n",
        "\n",
        "def sae_forward_logits(sae, y_raw_bO, mu_bo, std_bo):\n",
        "    dev = y_raw_bO.device\n",
        "    sae.eval().to(dev)\n",
        "    mu_t  = mu_bo.to(dev)  if isinstance(mu_bo,  torch.Tensor) else mu_bo\n",
        "    std_t = std_bo.to(dev) if isinstance(std_bo, torch.Tensor) else std_bo\n",
        "    y_n_bO = (y_raw_bO - mu_t) / std_t if isinstance(mu_t, torch.Tensor) else y_raw_bO\n",
        "    y_hat_n_bO, *_ = sae(y_n_bO)\n",
        "    if isinstance(mu_t, torch.Tensor):\n",
        "        y_hat_bO = y_hat_n_bO * std_t + mu_t\n",
        "    else:\n",
        "        y_hat_bO = y_hat_n_bO\n",
        "    return y_hat_bO\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_accuracy_with_sae_logits(model, loader, sae, mu_bo, std_bo):\n",
        "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval().to(dev); sae.eval().to(dev)\n",
        "    correct = total = 0\n",
        "    for x_bchw, y_gt in loader:\n",
        "        x_bi = x_bchw.to(dev).view(x_bchw.size(0), -1)\n",
        "        y_raw_bO = model.ffn(x_bi)\n",
        "        y_hat_bO = sae_forward_logits(sae, y_raw_bO, mu_bo, std_bo)\n",
        "        pred = y_hat_bO.argmax(dim=1)\n",
        "        correct += (pred == y_gt.to(dev)).sum().item()\n",
        "        total   += y_gt.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "\n",
        "def _trainer_sae(max_epochs):\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    has_bf16 = getattr(torch.cuda, \"is_bf16_supported\", lambda: False)()\n",
        "    precision = \"bf16-mixed\" if (use_gpu and has_bf16) else (16 if use_gpu else 32)\n",
        "    return pl.Trainer(\n",
        "        max_epochs=max_epochs,\n",
        "        accelerator=\"gpu\" if use_gpu else \"cpu\",\n",
        "        devices=1,\n",
        "        precision=precision,\n",
        "        logger=False,\n",
        "        enable_checkpointing=False,\n",
        "    )\n",
        "\n",
        "def train_sae_once(d_in, lam, Ytr, Yva, epochs):\n",
        "    sae = SAE_JumpReLU(d_in=d_in, d_latents=SAE_LATENTS, lambda_l0=lam, lr=LR_SAE_FINAL, init_theta=INIT_THETA, tau=TAU)\n",
        "    tr = _trainer_sae(epochs)\n",
        "    tr.fit(sae, tloader(Ytr, shuffle=True), tloader(Yva, shuffle=False))\n",
        "    return sae\n",
        "\n",
        "def calibrate_lambda(Ytr, Yva, target_actives, coarse_grid=np.geomspace(1e-6, 1e-1, 10), refine_factor=3, refine_steps=5):\n",
        "    d_in = Ytr.shape[1]\n",
        "    best = None\n",
        "    for lam in coarse_grid:\n",
        "        sae = train_sae_once(d_in, float(lam), Ytr, Yva, epochs=1)\n",
        "        m_act = gate_counts(sae, Yva)\n",
        "        gap = abs(m_act - target_actives)\n",
        "        if (best is None) or (gap < best[\"gap\"]):\n",
        "            best = {\"lam\": float(lam), \"sae\": sae, \"m_act\": float(m_act), \"gap\": float(gap)}\n",
        "    lam_star = best[\"lam\"]\n",
        "    low = lam_star / (refine_factor**2)\n",
        "    high = lam_star * (refine_factor**2)\n",
        "    refine_grid = np.geomspace(max(low, 1e-8), min(high, 1.0), refine_steps)\n",
        "    for lam in refine_grid:\n",
        "        sae = train_sae_once(d_in, float(lam), Ytr, Yva, epochs=1)\n",
        "        m_act = gate_counts(sae, Yva)\n",
        "        gap = abs(m_act - target_actives)\n",
        "        if gap < best[\"gap\"]:\n",
        "            best = {\"lam\": float(lam), \"sae\": sae, \"m_act\": float(m_act), \"gap\": float(gap)}\n",
        "    return best\n",
        "\n",
        "\n",
        "# device-safe overrides that work with both SAE variants (with .theta or .act.theta)\n",
        "def _get_theta(sae):\n",
        "    return sae.theta if hasattr(sae, \"theta\") else sae.act.theta\n",
        "\n",
        "def sae_forward_modes(sae, z_raw, mu, std, mode=\"jumprelu\"):\n",
        "    dev = z_raw.device\n",
        "    sae.eval().to(dev)\n",
        "    mu_t  = mu.to(dev)  if isinstance(mu,  torch.Tensor) else mu\n",
        "    std_t = std.to(dev) if isinstance(std, torch.Tensor) else std\n",
        "    z = (z_raw - mu_t) / std_t if isinstance(mu_t, torch.Tensor) else z_raw\n",
        "\n",
        "    u = sae.enc(z)\n",
        "    theta = _get_theta(sae)\n",
        "    if mode == \"jumprelu\":\n",
        "        b = (u > theta).float()\n",
        "        f = u * b\n",
        "    elif mode == \"boolean\":\n",
        "        f = (u > theta).float()\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'jumprelu' or 'boolean'\")\n",
        "\n",
        "    xh = sae.dec(f)\n",
        "    if isinstance(mu_t, torch.Tensor):\n",
        "        xh = xh * std_t + mu_t\n",
        "    return xh\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_accuracy_with_mode(model, loader, ffn_type, sae, mu, std, mode=\"jumprelu\"):\n",
        "    dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval().to(dev)\n",
        "    sae.eval().to(dev)\n",
        "\n",
        "    correct = total = 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(dev), yb.to(dev)\n",
        "        x_flat = xb.view(xb.size(0), -1)\n",
        "        z_raw  = torch.einsum('bi,ih->bh', x_flat, model.ffn.W_in)\n",
        "        z_hat  = sae_forward_modes(sae, z_raw, mu, std, mode=mode)\n",
        "\n",
        "        if ffn_type == \"ReLU\":\n",
        "            h = F.relu(z_hat)\n",
        "            logits = torch.einsum('bh,ho->bo', h, model.ffn.W_out)\n",
        "        else:\n",
        "            gate = F.gelu(torch.einsum('bi,ih->bh', x_flat, model.ffn.W_gate))\n",
        "            h = z_hat * gate\n",
        "            logits = torch.einsum('bh,ho->bo', h, model.ffn.W_out)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total   += yb.numel()\n",
        "    return correct / max(total, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9 — robust & fast SAE run (fixes Cell 9 crash)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# - Uses small subsets for calibration/final SAE\n",
        "# - Small lambda grid + few refine steps\n",
        "# - Try/except around long-running calls so interrupts don't explode the traceback\n",
        "\n",
        "# Speed knobs\n",
        "FAST_MODE = True\n",
        "CAL_SAMPLES_TRAIN = 8000 if FAST_MODE else len(Ytr_n)\n",
        "CAL_SAMPLES_VAL   = 2000 if FAST_MODE else len(Yva_n)\n",
        "COARSE_GRID       = np.geomspace(1e-5, 1e-2, 4)  # tiny grid\n",
        "REFINE_STEPS      = 3\n",
        "FINAL_EPOCHS      = min(EPOCHS_SAE_FINAL, 3) if FAST_MODE else EPOCHS_SAE_FINAL\n",
        "\n",
        "# Subsets for quick calibration/training\n",
        "Ytr_cal = Ytr_n[:CAL_SAMPLES_TRAIN].contiguous()\n",
        "Yva_cal = Yva_n[:CAL_SAMPLES_VAL].contiguous()\n",
        "print(f\"Calibrate on train={len(Ytr_cal)}, val={len(Yva_cal)}; coarse_grid={COARSE_GRID}\")\n",
        "\n",
        "results = []\n",
        "for target_k in TARGET_ACTIVES:\n",
        "    print(f\"\\n=== Calibrating for target actives ≈ {target_k} on LOGITS ===\")\n",
        "    # --- Calibration ---\n",
        "    try:\n",
        "        pick = calibrate_lambda(\n",
        "            Ytr_cal, Yva_cal, target_k,\n",
        "            coarse_grid=COARSE_GRID,\n",
        "            refine_steps=REFINE_STEPS\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Calibration interrupted — falling back to λ=1e-3\")\n",
        "        pick = {\"lam\": 1e-3, \"m_act\": float(\"nan\")}\n",
        "    print(f\"Picked lambda={pick['lam']:.2e}; achieved actives ≈ {pick.get('m_act', float('nan')):.2f} (cal)\")\n",
        "\n",
        "    # --- Train final SAE ---\n",
        "    try:\n",
        "        sae_final = train_sae_once(Ytr_cal.shape[1], pick[\"lam\"], Ytr_cal, Yva_cal, epochs=FINAL_EPOCHS)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Final SAE training interrupted — stopping cleanly.\")\n",
        "        raise  # re-raise so you can stop the run without a messy stacktrace\n",
        "\n",
        "    # Evaluate (use full val/test – cheap)\n",
        "    achieved_k   = gate_counts(sae_final, Yva_n)\n",
        "    acc_baseline = base_test_acc\n",
        "    acc_sae      = test_accuracy_with_sae_logits(model, dl_test, sae_final, mu_bo, std_bo)\n",
        "\n",
        "    row = {\n",
        "        \"target_actives\": target_k,\n",
        "        \"achieved_actives_val\": round(achieved_k, 2),\n",
        "        \"lambda\": pick[\"lam\"],\n",
        "        \"baseline_acc\": round(acc_baseline, 4),\n",
        "        \"recon_acc_logits\": round(acc_sae, 4),\n",
        "        \"delta_acc_logits\": round(acc_baseline - acc_sae, 4),\n",
        "        \"sae_latents\": SAE_LATENTS,\n",
        "        \"tau\": TAU,\n",
        "        \"normalize_logits\": NORMALIZE_Y,\n",
        "        \"batch_sizes\": {\"ffn\": BATCH_FFN, \"sae\": BATCH_SAE},\n",
        "    }\n",
        "    results.append(row)\n",
        "    print(row)\n",
        "\n",
        "# Persist results\n",
        "with open(os.path.join(OUT_DIR, \"summary.json\"), \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Pick & write the \"best\" row\n",
        "best = min(results, key=lambda r: (abs(r[\"target_actives\"]-1), r[\"delta_acc_logits\"]))\n",
        "best_md = (\n",
        "    \"# Best Result (Auto)\\n\"\n",
        "    f\"- target_actives: {best['target_actives']}\\n\"\n",
        "    f\"- achieved_actives_val: {best['achieved_actives_val']}\\n\"\n",
        "    f\"- lambda: {best['lambda']:.3e}\\n\"\n",
        "    f\"- baseline_acc: {best['baseline_acc']:.4f}\\n\"\n",
        "    f\"- recon_acc_logits: {best['recon_acc_logits']:.4f}\\n\"\n",
        "    f\"- delta_acc_logits: {best['delta_acc_logits']:.4f}\\n\"\n",
        "    f\"- sae_latents: {best['sae_latents']}\\n\"\n",
        "    f\"- tau: {best['tau']}\\n\"\n",
        "    f\"- normalize_logits: {best['normalize_logits']}\\n\"\n",
        "    f\"- batch_sizes: FFN={best['batch_sizes']['ffn']}, SAE={best['batch_sizes']['sae']}\\n\"\n",
        ")\n",
        "with open(os.path.join(OUT_DIR, \"BEST_RESULTS.md\"), \"w\") as f:\n",
        "    f.write(best_md)\n",
        "\n",
        "print(\"\\nWrote:\", os.path.join(OUT_DIR, \"summary.json\"))\n",
        "print(\"Wrote:\", os.path.join(OUT_DIR, \"BEST_RESULTS.md\"))\n",
        "\n",
        "\n",
        "best_path = os.path.join(OUT_DIR, \"BEST_RESULTS.md\")\n",
        "if os.path.exists(best_path):\n",
        "    with open(best_path, \"r\") as f:\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(\"Run Cell 9 first to generate BEST_RESULTS.md\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 12 — Consistent reconstruction-MSE evaluators (normalized vs unnormalized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def sae_forward_recon(sae, z_raw, mu=None, std=None, mode=\"jumprelu\"):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      z_hat_norm  : reconstruction in the *normalized* SAE space\n",
        "      z_recon_raw : reconstruction mapped back to *raw* (unnormalized) space (if mu/std given)\n",
        "    \"\"\"\n",
        "    dev = next(sae.parameters()).device\n",
        "    z_raw = z_raw.to(dev)\n",
        "\n",
        "    # normalize to the space used during SAE training\n",
        "    if (isinstance(mu, torch.Tensor) and isinstance(std, torch.Tensor)):\n",
        "        mu_t  = mu.to(dev)\n",
        "        std_t = std.to(dev)\n",
        "        z = (z_raw - mu_t) / (std_t + 1e-8)\n",
        "    else:\n",
        "        mu_t = std_t = None\n",
        "        z = z_raw\n",
        "\n",
        "    u = sae.enc(z)\n",
        "\n",
        "    # --- SAFE theta retrieval (no tensor truthiness) ---\n",
        "    theta = None\n",
        "    if hasattr(sae, \"theta_h\"):\n",
        "        theta = getattr(sae, \"theta_h\")\n",
        "    elif hasattr(sae, \"theta\"):\n",
        "        theta = getattr(sae, \"theta\")\n",
        "    if isinstance(theta, torch.nn.Parameter):\n",
        "        theta = theta.data\n",
        "    if theta is None:\n",
        "        theta = torch.tensor(0.0, device=dev, dtype=u.dtype)\n",
        "    else:\n",
        "        theta = theta.to(device=dev, dtype=u.dtype)\n",
        "    # ---------------------------------------------------\n",
        "\n",
        "    if mode == \"jumprelu\":\n",
        "        b = (u > theta).to(u.dtype)\n",
        "        f = u * b\n",
        "    elif mode == \"boolean\":\n",
        "        f = (u > theta).to(u.dtype)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "    z_hat_norm = sae.dec(f)  # normalized space\n",
        "\n",
        "    if (mu_t is not None) and (std_t is not None):\n",
        "        z_recon_raw = z_hat_norm * (std_t + 1e-8) + mu_t\n",
        "    else:\n",
        "        z_recon_raw = z_hat_norm\n",
        "\n",
        "    return z_hat_norm, z_recon_raw\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def mse_over_loader(sae, loader, mu=None, std=None, mode=\"jumprelu\", compare_space=\"normalized\"):\n",
        "    \"\"\"\n",
        "    compare_space: \"normalized\"  -> MSE between z_hat_norm and z_norm   (matches SAE training logs)\n",
        "                   \"raw\"         -> MSE between z_recon_raw and z_raw   (user-facing, intuitive)\n",
        "    Returns scalar mean MSE over all examples and dimensions.\n",
        "    \"\"\"\n",
        "    sae.eval()\n",
        "    dev = next(sae.parameters()).device\n",
        "    total_sqerr, total_count = 0.0, 0\n",
        "\n",
        "    for batch in loader:\n",
        "        # accept (tensor,) or tensor\n",
        "        z_raw = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "        z_raw = z_raw.to(dev)\n",
        "\n",
        "        # forward\n",
        "        z_hat_norm, z_recon_raw = sae_forward_recon(sae, z_raw, mu=mu, std=std, mode=mode)\n",
        "\n",
        "        if compare_space == \"normalized\":\n",
        "            if (isinstance(mu, torch.Tensor) and isinstance(std, torch.Tensor)):\n",
        "                z_norm = (z_raw - mu.to(dev)) / (std.to(dev) + 1e-8)\n",
        "            else:\n",
        "                z_norm = z_raw\n",
        "            diff = z_hat_norm - z_norm\n",
        "        elif compare_space == \"raw\":\n",
        "            diff = z_recon_raw - z_raw\n",
        "        else:\n",
        "            raise ValueError(\"compare_space must be 'normalized' or 'raw'\")\n",
        "\n",
        "        total_sqerr += diff.pow(2).sum().item()\n",
        "        total_count += diff.numel()\n",
        "\n",
        "    return total_sqerr / max(total_count, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 13 — Compute train/val/test MSE in normalized space (matches training logs) and raw space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reuse your existing hidden-activation tensors & loaders (the same reps you trained the SAE on):\n",
        "# Assuming you already have Ytr_bo, Yva_bo, Yte_bo and mu_bo, std_bo, sae_final\n",
        "train_loader = tloader(Ytr_bo, bs=256, shuffle=False)\n",
        "val_loader   = tloader(Yva_bo, bs=256, shuffle=False)\n",
        "test_loader  = tloader(Yte_bo, bs=256, shuffle=False)\n",
        "\n",
        "# 1) Normalized-space MSE (should be close to Lightning's train/val recon_mse ~ 0.2)\n",
        "tr_mse_norm = mse_over_loader(sae_final, train_loader, mu=mu_bo, std=std_bo, compare_space=\"normalized\")\n",
        "va_mse_norm = mse_over_loader(sae_final, val_loader,   mu=mu_bo, std=std_bo, compare_space=\"normalized\")\n",
        "te_mse_norm = mse_over_loader(sae_final, test_loader,  mu=mu_bo, std=std_bo, compare_space=\"normalized\")\n",
        "\n",
        "# 2) Raw-space MSE (often larger because it includes the original scale)\n",
        "tr_mse_raw = mse_over_loader(sae_final, train_loader, mu=mu_bo, std=std_bo, compare_space=\"raw\")\n",
        "va_mse_raw = mse_over_loader(sae_final, val_loader,   mu=mu_bo, std=std_bo, compare_space=\"raw\")\n",
        "te_mse_raw = mse_over_loader(sae_final, test_loader,  mu=mu_bo, std=std_bo, compare_space=\"raw\")\n",
        "\n",
        "print(f\"[Recon MSE — normalized space] train={tr_mse_norm:.6f} | val={va_mse_norm:.6f} | test={te_mse_norm:.6f}\")\n",
        "print(f\"[Recon MSE — raw space]        train={tr_mse_raw:.6f}  | val={va_mse_raw:.6f}  | test={te_mse_raw:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 14 — Optional: per-example MSE distribution and best/worst examples (on validation set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def per_example_mse(sae, Z_raw, mu=None, std=None, mode=\"jumprelu\", compare_space=\"normalized\", batch_size=512):\n",
        "    dev = next(sae.parameters()).device\n",
        "    sae.eval()\n",
        "    N = Z_raw.shape[0]\n",
        "    out = torch.empty(N, device=\"cpu\")\n",
        "    for i in range(0, N, batch_size):\n",
        "        z_batch = Z_raw[i:i+batch_size].to(dev)\n",
        "        z_hat_norm, z_recon_raw = sae_forward_recon(sae, z_batch, mu=mu, std=std, mode=mode)\n",
        "        if compare_space == \"normalized\":\n",
        "            z_norm = (z_batch - mu.to(dev)) / (std.to(dev) + 1e-8) if (isinstance(mu, torch.Tensor) and isinstance(std, torch.Tensor)) else z_batch\n",
        "            diff = z_hat_norm - z_norm\n",
        "        else:\n",
        "            diff = z_recon_raw - z_batch\n",
        "        out[i:i+batch_size] = diff.pow(2).mean(dim=1).detach().cpu()\n",
        "    return out  # (N,)\n",
        "\n",
        "# Compute per-example val MSE in normalized space\n",
        "val_per_ex_mse = per_example_mse(sae_final, Yva_bo, mu=mu_bo, std=std_bo, compare_space=\"normalized\")\n",
        "val_avg = val_per_ex_mse.mean().item()\n",
        "val_med = val_per_ex_mse.median().item()\n",
        "val_p95 = val_per_ex_mse.quantile(0.95).item()\n",
        "best_idx = int(torch.argmin(val_per_ex_mse))\n",
        "worst_idx = int(torch.argmax(val_per_ex_mse))\n",
        "\n",
        "print(f\"[Val per-example MSE — normalized] mean={val_avg:.6f} | median={val_med:.6f} | p95={val_p95:.6f}\")\n",
        "print(f\"Best example idx={best_idx} mse={val_per_ex_mse[best_idx].item():.6f}\")\n",
        "print(f\"Worst example idx={worst_idx} mse={val_per_ex_mse[worst_idx].item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 15 — CE/accuracy helpers that work with your loaders and model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "@torch.no_grad()\n",
        "def ce_and_acc_over_img_loader(model, loader, device=None):\n",
        "    dev = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "    model.eval().to(dev)\n",
        "    ce_sum, correct, seen = 0.0, 0, 0\n",
        "    for x_bchw, y_gt in loader:\n",
        "        x_bchw = x_bchw.to(dev)\n",
        "        y_gt   = y_gt.to(dev).long()\n",
        "        logits = model(x_bchw)                # baseline logits\n",
        "        ce_sum += F.cross_entropy(logits, y_gt, reduction=\"sum\").item()\n",
        "        correct += (logits.argmax(dim=1) == y_gt).sum().item()\n",
        "        seen += y_gt.numel()\n",
        "    return ce_sum / max(seen, 1), correct / max(seen, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def ce_and_acc_over_img_loader_with_sae(model, loader, sae, mu_bo, std_bo, device=None):\n",
        "    dev = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "    model.eval().to(dev); sae.eval().to(dev)\n",
        "    ce_sum, correct, seen = 0.0, 0, 0\n",
        "    for x_bchw, y_gt in loader:\n",
        "        x_bchw = x_bchw.to(dev)\n",
        "        y_gt   = y_gt.to(dev).long()\n",
        "        # your pipeline: x -> MLP.ffn logits -> SAE -> recon logits\n",
        "        x_bi      = x_bchw.view(x_bchw.size(0), -1)\n",
        "        y_raw_bO  = model.ffn(x_bi)\n",
        "        y_hat_bO  = sae_forward_logits(sae, y_raw_bO, mu_bo, std_bo)  # already returns RAW-space recon logits\n",
        "        ce_sum   += F.cross_entropy(y_hat_bO, y_gt, reduction=\"sum\").item()\n",
        "        correct  += (y_hat_bO.argmax(dim=1) == y_gt).sum().item()\n",
        "        seen     += y_gt.numel()\n",
        "    return ce_sum / max(seen, 1), correct / max(seen, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 16 — Compute CE/Acc for baseline vs SAE on all splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline (no SAE)\n",
        "tr_ce_base, tr_acc_base = ce_and_acc_over_img_loader(model, dl_train, device=device)\n",
        "va_ce_base, va_acc_base = ce_and_acc_over_img_loader(model, dl_val,   device=device)\n",
        "te_ce_base, te_acc_base = ce_and_acc_over_img_loader(model, dl_test,  device=device)\n",
        "\n",
        "# With SAE-reconstructed logits\n",
        "tr_ce_sae, tr_acc_sae = ce_and_acc_over_img_loader_with_sae(model, dl_train, sae_final, mu_bo, std_bo, device=device)\n",
        "va_ce_sae, va_acc_sae = ce_and_acc_over_img_loader_with_sae(model, dl_val,   sae_final, mu_bo, std_bo, device=device)\n",
        "te_ce_sae, te_acc_sae = ce_and_acc_over_img_loader_with_sae(model, dl_test,  sae_final, mu_bo, std_bo, device=device)\n",
        "\n",
        "print(\"=== Cross-Entropy (↓) / Accuracy (↑) ===\")\n",
        "print(f\"TRAIN | CE base {tr_ce_base:.4f} | CE SAE {tr_ce_sae:.4f} | ΔCE {tr_ce_sae - tr_ce_base:+.4f} | Acc base {tr_acc_base:.4f} | Acc SAE {tr_acc_sae:.4f} | ΔAcc {tr_acc_sae - tr_acc_base:+.4f}\")\n",
        "print(f\"VAL   | CE base {va_ce_base:.4f} | CE SAE {va_ce_sae:.4f} | ΔCE {va_ce_sae - va_ce_base:+.4f} | Acc base {va_acc_base:.4f} | Acc SAE {va_acc_sae:.4f} | ΔAcc {va_acc_sae - va_acc_base:+.4f}\")\n",
        "print(f\"TEST  | CE base {te_ce_base:.4f} | CE SAE {te_ce_sae:.4f} | ΔCE {te_ce_sae - te_ce_base:+.4f} | Acc base {te_acc_base:.4f} | Acc SAE {te_acc_sae:.4f} | ΔAcc {te_acc_sae - te_acc_base:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 17 — MLP sweep: configs + train/eval helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import itertools, json, os\n",
        "from pathlib import Path\n",
        "\n",
        "SWEEP_HIDDEN = [1024, 8192]\n",
        "SWEEP_LR     = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "SWEEP_EPOCHS = EPOCHS_FFN            # keep same epochs; bump if you want even lower CE\n",
        "\n",
        "assert BATCH_FFN == 8, f\"BATCH_FFN is {BATCH_FFN}, mentor requested bs=8. Recreate loaders with batch_size=8.\"\n",
        "# (Recreate anyway to be safe)\n",
        "dl_train, dl_val, dl_test = load_mnist(batch_size=8)\n",
        "\n",
        "SWEEP_DIR = Path(OUT_DIR) / \"ffn_sweep\"\n",
        "SWEEP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _acc_over_loader(model, loader, device=None):\n",
        "    dev = device or next(model.parameters()).device\n",
        "    model.eval().to(dev)\n",
        "    correct=total=0\n",
        "    for x_bchw, y_gt in loader:\n",
        "        y_bo = model(x_bchw.to(dev))\n",
        "        pred = y_bo.argmax(dim=1)\n",
        "        correct += (pred == y_gt.to(dev)).sum().item()\n",
        "        total   += y_gt.numel()\n",
        "    return correct / max(total,1)\n",
        "\n",
        "def train_eval_ffn(d_h, lr, epochs=SWEEP_EPOCHS, ffn_type=FFN_TYPE):\n",
        "    # fresh model for each run\n",
        "    m = MNIST_FFN(ffn_type, d_h=d_h, lr=lr)\n",
        "    tr = _trainer(epochs)\n",
        "    tr.fit(m, dl_train, dl_val)\n",
        "\n",
        "    # CE/Acc on splits\n",
        "    tr_ce, tr_acc = ce_and_acc_over_img_loader(m, dl_train, device=device)\n",
        "    va_ce, va_acc = ce_and_acc_over_img_loader(m, dl_val,   device=device)\n",
        "    te_ce, te_acc = ce_and_acc_over_img_loader(m, dl_test,  device=device)\n",
        "\n",
        "    # also baseline test acc via your original helper (just for parity)\n",
        "    te_acc2 = _acc_over_loader(m, dl_test, device=device)\n",
        "\n",
        "    return {\n",
        "        \"hidden_dim\": d_h,\n",
        "        \"lr\": lr,\n",
        "        \"epochs\": epochs,\n",
        "        \"train_ce\": tr_ce, \"train_acc\": tr_acc,\n",
        "        \"val_ce\": va_ce,   \"val_acc\": va_acc,\n",
        "        \"test_ce\": te_ce,  \"test_acc\": te_acc,\n",
        "        \"test_acc_alt\": te_acc2,\n",
        "        \"ffn_type\": ffn_type,\n",
        "    }, m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 18 — Execute sweep and pick best by lowest validation CE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _lr_tag(x):  # safe filename tag for LRs\n",
        "    s = f\"{x:.0e}\".replace(\"+\", \"\").replace(\"-\", \"m\")\n",
        "    return s\n",
        "\n",
        "sweep_results = []\n",
        "best = None\n",
        "best_model = None\n",
        "\n",
        "print(\"=== MLP Sweep (bs=8) ===\")\n",
        "for d_h, lr in itertools.product(SWEEP_HIDDEN, SWEEP_LR):\n",
        "    tag = f\"d{d_h}_lr{_lr_tag(lr)}\"\n",
        "    print(f\"\\n-> Training MLP: hidden={d_h}, lr={lr} ({tag})\")\n",
        "    res, m = train_eval_ffn(d_h, lr)\n",
        "\n",
        "    # save checkpoint & metrics\n",
        "    ckpt_path = SWEEP_DIR / f\"ffn_{tag}.pt\"\n",
        "    torch.save(m.state_dict(), ckpt_path)\n",
        "    res[\"ckpt\"] = str(ckpt_path)\n",
        "    sweep_results.append(res)\n",
        "\n",
        "    # print row\n",
        "    print(f\"   Train CE {res['train_ce']:.4f} | Val CE {res['val_ce']:.4f} | Test CE {res['test_ce']:.4f}\")\n",
        "    print(f\"   Train Acc {res['train_acc']:.4f} | Val Acc {res['val_acc']:.4f} | Test Acc {res['test_acc']:.4f}\")\n",
        "\n",
        "    # track best by lowest val CE\n",
        "    if (best is None) or (res[\"val_ce\"] < best[\"val_ce\"]):\n",
        "        best = deepcopy(res)\n",
        "        best_model = deepcopy(m)  # keep in-memory copy of best\n",
        "\n",
        "# Save summary\n",
        "sum_path = SWEEP_DIR / \"summary_ffn_sweep.json\"\n",
        "with open(sum_path, \"w\") as f:\n",
        "    json.dump(sweep_results, f, indent=2)\n",
        "\n",
        "print(\"\\n=== Sweep complete ===\")\n",
        "print(f\"Saved summary: {sum_path}\")\n",
        "print(\"Top-3 by Val CE:\")\n",
        "top3 = sorted(sweep_results, key=lambda r: r[\"val_ce\"])[:3]\n",
        "for i, r in enumerate(top3, 1):\n",
        "    print(f\"{i}) d={r['hidden_dim']:<5} lr={r['lr']:<8} | Val CE={r['val_ce']:.4f} | Val Acc={r['val_acc']:.4f} | ckpt={Path(r['ckpt']).name}\")\n",
        "\n",
        "print(\"\\nBest (Val CE):\")\n",
        "print(best)\n",
        "best_ckpt = best[\"ckpt\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 19 — Transcoder config (fixed hidden dim = 1024)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HIDDEN_DIM_TX = 1024\n",
        "BATCH_TX      = 8\n",
        "EPOCHS_TX_MLP = 3\n",
        "LR_TX_MLP     = 1e-3\n",
        "\n",
        "TX_LR     = 1e-3\n",
        "TX_WD     = 1e-4\n",
        "TX_EPOCHS = 5\n",
        "TX_BS     = 256\n",
        "\n",
        "TX_DIR = os.path.join(OUT_DIR, \"transcoder_1024\")\n",
        "os.makedirs(TX_DIR, exist_ok=True)\n",
        "\n",
        "dl_train_tx, dl_val_tx, dl_test_tx = load_mnist(batch_size=BATCH_TX)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 20 — Train two 1024-wide MLPs A and B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_ffn_with_seed(seed, ffn_type=\"ReLU\", hidden=HIDDEN_DIM_TX, lr=LR_TX_MLP, epochs=EPOCHS_TX_MLP):\n",
        "    pl.seed_everything(seed, workers=True)\n",
        "    m = MNIST_FFN(ffn_type=ffn_type, d_h=hidden, lr=lr)\n",
        "    tr = _trainer(epochs)\n",
        "    tr.fit(m, dl_train_tx, dl_val_tx)\n",
        "    return m\n",
        "\n",
        "modelA = train_ffn_with_seed(123, hidden=HIDDEN_DIM_TX)\n",
        "modelB = train_ffn_with_seed(456, hidden=HIDDEN_DIM_TX)\n",
        "\n",
        "@torch.no_grad()\n",
        "def ce_acc(model, loader):\n",
        "    ce, acc = ce_and_acc_over_img_loader(model, loader, device=device)\n",
        "    return ce, acc\n",
        "\n",
        "print(\"Model A 1024 | Val CE/Acc:\", ce_acc(modelA, dl_val_tx))\n",
        "print(\"Model B 1024 | Val CE/Acc:\", ce_acc(modelB, dl_val_tx))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 21 — Collect hidden preactivations z = x W_in for A and B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def collect_preacts(model, loader):\n",
        "    model.eval().to(device)\n",
        "    Z = []\n",
        "    for xb, _ in loader:\n",
        "        xb = xb.to(device)\n",
        "        x_flat = xb.view(xb.size(0), -1)\n",
        "        z = torch.einsum('bi,ih->bh', x_flat, model.ffn.W_in_ih)\n",
        "        Z.append(z.cpu())\n",
        "    return torch.cat(Z, dim=0)\n",
        "\n",
        "ZA_tr, ZB_tr = collect_preacts(modelA, dl_train_tx), collect_preacts(modelB, dl_train_tx)\n",
        "ZA_va, ZB_va = collect_preacts(modelA, dl_val_tx),   collect_preacts(modelB, dl_val_tx)\n",
        "ZA_te, ZB_te = collect_preacts(modelA, dl_test_tx),  collect_preacts(modelB, dl_test_tx)\n",
        "print(\"Train shapes:\", ZA_tr.shape, ZB_tr.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 22 — Standardize A and B spaces and build z loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "muA, stdA = ZA_tr.mean(0, keepdim=True), ZA_tr.std(0, keepdim=True).clamp_min(1e-6)\n",
        "muB, stdB = ZB_tr.mean(0, keepdim=True), ZB_tr.std(0, keepdim=True).clamp_min(1e-6)\n",
        "\n",
        "def znorm(Z, mu, std): return (Z - mu) / std\n",
        "ZA_tr_n, ZB_tr_n = znorm(ZA_tr, muA, stdA), znorm(ZB_tr, muB, stdB)\n",
        "ZA_va_n, ZB_va_n = znorm(ZA_va, muA, stdA), znorm(ZB_va, muB, stdB)\n",
        "ZA_te_n, ZB_te_n = znorm(ZA_te, muA, stdA), znorm(ZB_te, muB, stdB)\n",
        "\n",
        "def zloader(ZA, ZB, bs=TX_BS, shuffle=True):\n",
        "    return DataLoader(TensorDataset(ZA, ZB), batch_size=bs, shuffle=shuffle,\n",
        "                      num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "dl_z_tr = zloader(ZA_tr_n, ZB_tr_n, bs=TX_BS, shuffle=True)\n",
        "dl_z_va = zloader(ZA_va_n, ZB_va_n, bs=TX_BS, shuffle=False)\n",
        "dl_z_te = zloader(ZA_te_n, ZB_te_n, bs=TX_BS, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 23 — Train linear transcoder T: A→B in normalized space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transcoder(nn.Module):\n",
        "    def __init__(self, d):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(d, d, bias=True)\n",
        "        nn.init.zeros_(self.lin.bias)\n",
        "    def forward(self, zA_n):\n",
        "        return self.lin(zA_n)\n",
        "\n",
        "def train_transcoder(d, dl_train, dl_val, lr=TX_LR, wd=TX_WD, epochs=TX_EPOCHS):\n",
        "    T = Transcoder(d).to(device)\n",
        "    opt = torch.optim.Adam(T.parameters(), lr=lr, weight_decay=wd)\n",
        "    best_va = float('inf'); best = None\n",
        "    for ep in range(1, epochs+1):\n",
        "        T.train(); tr_loss=0.0; n=0\n",
        "        for a_n, b_n in dl_train:\n",
        "            a_n, b_n = a_n.to(device), b_n.to(device)\n",
        "            loss = F.mse_loss(T(a_n), b_n)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            tr_loss += loss.item()*a_n.size(0); n += a_n.size(0)\n",
        "        tr_loss /= max(n,1)\n",
        "        T.eval(); va_loss=0.0; n=0\n",
        "        with torch.no_grad():\n",
        "            for a_n, b_n in dl_val:\n",
        "                a_n, b_n = a_n.to(device), b_n.to(device)\n",
        "                va_loss += F.mse_loss(T(a_n), b_n).item()*a_n.size(0); n += a_n.size(0)\n",
        "        va_loss /= max(n,1)\n",
        "        print(f\"Transcoder ep {ep}/{epochs} | train MSE {tr_loss:.6f} | val MSE {va_loss:.6f}\")\n",
        "        if va_loss < best_va:\n",
        "            best_va = va_loss\n",
        "            best = {k:v.detach().cpu().clone() for k,v in T.state_dict().items()}\n",
        "    if best is not None: T.load_state_dict(best)\n",
        "    return T, best_va\n",
        "\n",
        "D = HIDDEN_DIM_TX\n",
        "T_AB, best_va_mse = train_transcoder(D, dl_z_tr, dl_z_va)\n",
        "torch.save({\"state_dict\": T_AB.state_dict(), \"muA\": muA, \"stdA\": stdA, \"muB\": muB, \"stdB\": stdB},\n",
        "           os.path.join(TX_DIR, \"transcoder_A_to_B.pt\"))\n",
        "print(\"Saved:\", os.path.join(TX_DIR, \"transcoder_A_to_B.pt\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 24 — Evaluate transcoder MSE in normalized and raw spaces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def transcoder_mse(T, ZA, ZB, muA, stdA, muB, stdB, batch=2048):\n",
        "    T.eval().to(device)\n",
        "    total_n=total_r=0.0; count=0\n",
        "    for i in range(0, ZA.size(0), batch):\n",
        "        a, b = ZA[i:i+batch].to(device), ZB[i:i+batch].to(device)\n",
        "        a_n = (a - muA.to(device)) / (stdA.to(device) + 1e-8)\n",
        "        b_n = (b - muB.to(device)) / (stdB.to(device) + 1e-8)\n",
        "        b_hat_n = T(a_n)\n",
        "        diff_n = b_hat_n - b_n\n",
        "        mse_n  = diff_n.pow(2).mean().item()\n",
        "        b_hat  = b_hat_n * (stdB.to(device) + 1e-8) + muB.to(device)\n",
        "        diff_r = b_hat - b\n",
        "        mse_r  = diff_r.pow(2).mean().item()\n",
        "        total_n += mse_n * diff_n.numel()\n",
        "        total_r += mse_r * diff_r.numel()\n",
        "        count   += diff_n.numel()\n",
        "    return total_n/count, total_r/count\n",
        "\n",
        "va_mse_n, va_mse_r = transcoder_mse(T_AB, ZA_va, ZB_va, muA, stdA, muB, stdB)\n",
        "te_mse_n, te_mse_r = transcoder_mse(T_AB, ZA_te, ZB_te, muA, stdA, muB, stdB)\n",
        "print(f\"Transcoder MSE normalized | val {va_mse_n:.6f} | test {te_mse_n:.6f}\")\n",
        "print(f\"Transcoder MSE raw        | val {va_mse_r:.6f} | test {te_mse_r:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 25 — Functional test: substitute T(A) into B and score CE and Acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def ce_acc_B_with_transcoder(A, B, T, loader, muA, stdA, muB, stdB):\n",
        "    A.eval().to(device); B.eval().to(device); T.eval().to(device)\n",
        "    ce_sum=0.0; correct=0; seen=0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device).long()\n",
        "        x_flat = xb.view(xb.size(0), -1)\n",
        "        zA = torch.einsum('bi,ih->bh', x_flat, A.ffn.W_in_ih)\n",
        "        zA_n = (zA - muA.to(device)) / (stdA.to(device) + 1e-8)\n",
        "        zB_hat_n = T(zA_n)\n",
        "        zB_hat = zB_hat_n * (stdB.to(device) + 1e-8) + muB.to(device)\n",
        "        hB = F.relu(zB_hat)\n",
        "        logits_hat = torch.einsum('bh,ho->bo', hB, B.ffn.W_out_ho)\n",
        "        ce_sum += F.cross_entropy(logits_hat, yb, reduction=\"sum\").item()\n",
        "        correct += (logits_hat.argmax(dim=1) == yb).sum().item()\n",
        "        seen += yb.numel()\n",
        "    return ce_sum / max(seen,1), correct / max(seen,1)\n",
        "\n",
        "ceB_val, accB_val = ce_acc(modelB, dl_val_tx)\n",
        "ceB_te,  accB_te  = ce_acc(modelB, dl_test_tx)\n",
        "ceTX_val, accTX_val = ce_acc_B_with_transcoder(modelA, modelB, T_AB, dl_val_tx, muA, stdA, muB, stdB)\n",
        "ceTX_te,  accTX_te  = ce_acc_B_with_transcoder(modelA, modelB, T_AB, dl_test_tx, muA, stdA, muB, stdB)\n",
        "\n",
        "print(\"Functional fidelity T(A)→B\")\n",
        "print(f\"VAL  | CE base {ceB_val:.4f} | CE T {ceTX_val:.4f} | ΔCE {ceTX_val - ceB_val:+.4f} | Acc base {accB_val:.4f} | Acc T {accTX_val:.4f} | ΔAcc {accTX_val - accB_val:+.4f}\")\n",
        "print(f\"TEST | CE base {ceB_te:.4f}  | CE T {ceTX_te:.4f}  | ΔCE {ceTX_te - ceB_te:+.4f}  | Acc base {accB_te:.4f}  | Acc T {accTX_te:.4f}  | ΔAcc {accTX_te - accB_te:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 26 — Collect input x and target logits h from *the same MLP* (modelA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NORMALIZE_TX = True         # standardize targets per-dim (recommended)\n",
        "TX_LATENTS   = 128          # latent width for sparse T(x) -> h\n",
        "TX_TAU       = 0.1          # STE temperature\n",
        "TX_INIT_TH   = 0.5          # init threshold\n",
        "TX_LAMBDA    = 1e-2         # L0 (soft) weight\n",
        "TX_LR        = 1e-3\n",
        "TX_EPOCHS    = 5\n",
        "TX_BS        = 256\n",
        "\n",
        "assert 'modelA' in globals(), \"modelA (1024-wide MLP) not found. Train it first with your Cell 20.\"\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_x_and_logits(model, loader):\n",
        "    model.eval().to(device)\n",
        "    X_list, H_list = [], []\n",
        "    for xb, _ in loader:\n",
        "        xb = xb.to(device)\n",
        "        # x_flat in [0,1], shape [B, 784]\n",
        "        x_flat = xb.view(xb.size(0), -1)\n",
        "        # h = logits = W_out @ ReLU(W_in @ x)\n",
        "        h = model(xb)  # logits (pre-softmax)\n",
        "        X_list.append(x_flat.cpu())\n",
        "        H_list.append(h.cpu())\n",
        "    return torch.cat(X_list, 0), torch.cat(H_list, 0)\n",
        "\n",
        "Xtr, Htr = collect_x_and_logits(modelA, dl_train_tx)\n",
        "Xva, Hva = collect_x_and_logits(modelA, dl_val_tx)\n",
        "Xte, Hte = collect_x_and_logits(modelA, dl_test_tx)\n",
        "print(\"Shapes — X:\", Xtr.shape, Xva.shape, Xte.shape, \"  H:\", Htr.shape, Hva.shape, Hte.shape)\n",
        "\n",
        "# Standardize targets (logits) per-dimension; inputs x are already in [0,1]\n",
        "if NORMALIZE_TX:\n",
        "    mu_h  = Htr.mean(0, keepdim=True)\n",
        "    std_h = Htr.std(0, keepdim=True).clamp_min(1e-6)\n",
        "    Htr_n = (Htr - mu_h) / std_h\n",
        "    Hva_n = (Hva - mu_h) / std_h\n",
        "    Hte_n = (Hte - mu_h) / std_h\n",
        "else:\n",
        "    mu_h, std_h = None, None\n",
        "    Htr_n, Hva_n, Hte_n = Htr, Hva, Hte\n",
        "\n",
        "def loader_xh(X, H, bs=TX_BS, shuffle=True):\n",
        "    pin = torch.cuda.is_available()\n",
        "    return DataLoader(TensorDataset(X, H), batch_size=bs, shuffle=shuffle, num_workers=2, pin_memory=pin)\n",
        "\n",
        "dl_tx_tr = loader_xh(Xtr, Htr_n, bs=TX_BS, shuffle=True)\n",
        "dl_tx_va = loader_xh(Xva, Hva_n, bs=TX_BS, shuffle=False)\n",
        "dl_tx_te = loader_xh(Xte, Hte_n, bs=TX_BS, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 27 — JumpReLU Transcoder: x->[latent]->h (logits), with L0 via hard gate count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TranscoderJumpReLU(pl.LightningModule):\n",
        "    def __init__(self, d_in=28*28, d_lat=TX_LATENTS, d_out=10,\n",
        "                 tau=TX_TAU, init_theta=TX_INIT_TH, lambda_l0=TX_LAMBDA, lr=TX_LR):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.enc = nn.Linear(d_in, d_lat, bias=True)\n",
        "        self.theta_h = nn.Parameter(torch.full((d_lat,), float(init_theta)))\n",
        "        self.dec = nn.Linear(d_lat, d_out, bias=True)\n",
        "        self.tau = tau\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = self.enc(x)                                         # pre-gate\n",
        "        soft = torch.sigmoid((u - self.theta_h) / self.tau)     # surrogate\n",
        "        hard = (u > self.theta_h).float()                       # hard gate\n",
        "        gate = (hard - soft).detach() + soft                    # STE\n",
        "        f = u * gate                                            # sparse latents\n",
        "        h_hat = self.dec(f)                                     # predict logits (normalized space if we trained on normalized)\n",
        "        return h_hat, f, u, soft, hard\n",
        "\n",
        "    def _step(self, batch):\n",
        "        x, h_tgt = batch\n",
        "        h_hat, f, u, soft, hard = self(x)\n",
        "        recon = F.mse_loss(h_hat, h_tgt, reduction=\"mean\")\n",
        "        l0_soft = soft.sum(dim=1).mean()\n",
        "        loss = recon + self.hparams.lambda_l0 * l0_soft\n",
        "        l0_hard = hard.sum(dim=1).float().mean().detach()\n",
        "        logs = {\"recon_mse\": recon.detach(), \"l0_soft\": l0_soft.detach(), \"l0_hard\": l0_hard}\n",
        "        return loss, logs\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        loss, logs = self._step(batch)\n",
        "        self.log_dict({f\"train/{k}\": v for k,v in logs.items()}, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, _):\n",
        "        _, logs = self._step(batch)\n",
        "        self.log_dict({f\"val/{k}\": v for k,v in logs.items()}, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "def train_transcoder_x_to_h():\n",
        "    T = TranscoderJumpReLU(d_in=28*28, d_lat=TX_LATENTS, d_out=10)\n",
        "    tr = _trainer(TX_EPOCHS)\n",
        "    tr.fit(T, dl_tx_tr, dl_tx_va)\n",
        "    return T\n",
        "\n",
        "T_xh = train_transcoder_x_to_h()\n",
        "\n",
        "\n",
        "# Task: Implement a transcoder model that takes the input `x` of a given MLP and predicts the hidden pre-activations `h` of that MLP.\n",
        "# The implementation should include reconstruction error (MSE) and L0 sparsity regularization.\n",
        "# Modify the existing code cells to reflect this new transcoder definition and training process.\n",
        "\n",
        "# ## Modify data loading for transcoder\n",
        "\n",
        "# ### Subtask:\n",
        "# Update the data loading to provide pairs of original input `x` and hidden pre-activations `h` from a single trained MLP.\n",
        "\n",
        "# **Reasoning**:\n",
        "# The subtask requires collecting pairs of input images and hidden pre-activations from a single MLP.\n",
        "# The new function `collect_input_preacts` is needed to achieve this, and then it will be called for the train,\n",
        "# validation, and test sets using `modelA` and the corresponding data loaders. Finally, the shapes will be printed to verify the output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 28 — Report reconstruction error (MSE) and L0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mse_split(T, X, H, mu_h=None, std_h=None, bs=1024):\n",
        "    T.eval().to(device)\n",
        "    total_n = total_r = 0.0\n",
        "    cnt_n = cnt_r = 0\n",
        "    for i in range(0, X.size(0), bs):\n",
        "        x = X[i:i+bs].to(device)\n",
        "        h_raw = H[i:i+bs].to(device)\n",
        "        # normalized targets used in training\n",
        "        h_tgt_n = (h_raw - mu_h.to(device)) / (std_h.to(device) + 1e-8) if (mu_h is not None) else h_raw\n",
        "        h_hat_n, *_ = T(x.to(device))\n",
        "        # normalized MSE\n",
        "        diff_n = h_hat_n - h_tgt_n\n",
        "        total_n += diff_n.pow(2).sum().item()\n",
        "        cnt_n   += diff_n.numel()\n",
        "        # raw MSE (map prediction back if normalized training)\n",
        "        if mu_h is not None:\n",
        "            h_hat_raw = h_hat_n * (std_h.to(device) + 1e-8) + mu_h.to(device)\n",
        "        else:\n",
        "            h_hat_raw = h_hat_n\n",
        "        diff_r = h_hat_raw - h_raw\n",
        "        total_r += diff_r.pow(2).sum().item()\n",
        "        cnt_r   += diff_r.numel()\n",
        "    return total_n/max(cnt_n,1), total_r/max(cnt_r,1)\n",
        "\n",
        "tr_mse_n, tr_mse_r = mse_split(T_xh, Xtr, Htr, mu_h, std_h)\n",
        "va_mse_n, va_mse_r = mse_split(T_xh, Xva, Hva, mu_h, std_h)\n",
        "te_mse_n, te_mse_r = mse_split(T_xh, Xte, Hte, mu_h, std_h)\n",
        "\n",
        "print(f\"[T(x)->h MSE — normalized] train={tr_mse_n:.6f} | val={va_mse_n:.6f} | test={te_mse_n:.6f}\")\n",
        "print(f\"[T(x)->h MSE — raw]        train={tr_mse_r:.6f}  | val={va_mse_r:.6f}  | test={te_mse_r:.6f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def mean_hard_actives(T, X, bs=2048):\n",
        "    T.eval().to(device)\n",
        "    acts = []\n",
        "    for i in range(0, X.size(0), bs):\n",
        "        x = X[i:i+bs].to(device)\n",
        "        _, _, u, _, _ = T(x)\n",
        "        acts.append((u > T.theta_h).float().sum(dim=1).cpu())\n",
        "    acts = torch.cat(acts, 0)\n",
        "    return float(acts.mean()), float(acts.median()), float(acts.quantile(0.95))\n",
        "\n",
        "l0_mean, l0_med, l0_p95 = mean_hard_actives(T_xh, Xva)\n",
        "print(f\"[T(x)->h L0 (hard gate count) — VAL] mean={l0_mean:.2f} | median={l0_med:.2f} | p95={l0_p95:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 29 — Optional fidelity: use T(x) logits directly to score CE/Acc vs labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def ce_acc_with_T_logits(loader, T, mu_h=None, std_h=None):\n",
        "    T.eval().to(device)\n",
        "    ce_sum=0.0; correct=0; seen=0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device).long()\n",
        "        x_flat = xb.view(xb.size(0), -1)\n",
        "        h_hat_n, _f, _u, _s, _h = T(x_flat)\n",
        "        logits = h_hat_n * (std_h.to(device) + 1e-8) + mu_h.to(device) if (mu_h is not None) else h_hat_n\n",
        "        ce_sum += F.cross_entropy(logits, yb, reduction=\"sum\").item()\n",
        "        correct += (logits.argmax(dim=1) == yb).sum().item()\n",
        "        seen += yb.numel()\n",
        "    return ce_sum/max(seen,1), correct/max(seen,1)\n",
        "\n",
        "# Baseline CE/Acc from modelA logits\n",
        "tr_ce_base, tr_acc_base = ce_and_acc_over_img_loader(modelA, dl_train_tx, device=device)\n",
        "va_ce_base, va_acc_base = ce_and_acc_over_img_loader(modelA, dl_val_tx,   device=device)\n",
        "te_ce_base, te_acc_base = ce_and_acc_over_img_loader(modelA, dl_test_tx,  device=device)\n",
        "\n",
        "# CE/Acc from T(x) logits\n",
        "tr_ce_T, tr_acc_T = ce_acc_with_T_logits(dl_train_tx, T_xh, mu_h, std_h)\n",
        "va_ce_T, va_acc_T = ce_acc_with_T_logits(dl_val_tx,   T_xh, mu_h, std_h)\n",
        "te_ce_T, te_acc_T = ce_acc_with_T_logits(dl_test_tx,  T_xh, mu_h, std_h)\n",
        "\n",
        "print(\"=== CE/Acc using T(x) logits ===\")\n",
        "print(f\"TRAIN | CE base {tr_ce_base:.4f} | CE T {tr_ce_T:.4f} | ΔCE {tr_ce_T - tr_ce_base:+.4f} | Acc base {tr_acc_base:.4f} | Acc T {tr_acc_T:.4f} | ΔAcc {tr_acc_T - tr_acc_base:+.4f}\")\n",
        "print(f\"VAL   | CE base {va_ce_base:.4f} | CE T {va_ce_T:.4f} | ΔCE {va_ce_T - va_ce_base:+.4f} | Acc base {va_acc_base:.4f} | Acc T {va_acc_T:.4f} | ΔAcc {va_acc_T - va_acc_base:+.4f}\")\n",
        "print(f\"TEST  | CE base {te_ce_base:.4f} | CE T {te_ce_T:.4f} | ΔCE {te_ce_T - te_ce_base:+.4f} | Acc base {te_acc_base:.4f} | Acc T {te_acc_T:.4f} | ΔAcc {te_acc_T - te_acc_base:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 30 — Collect (h1, h2) from modelA (d=1024 → 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# h1 = ReLU(W_in @ x), h2 = W_out @ h1\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_h1_h2(model, loader):\n",
        "    model.eval().to(device)\n",
        "    H1, H2, Y = [], [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device).long()\n",
        "        x_flat = xb.view(xb.size(0), -1)\n",
        "        z  = torch.einsum('bi,ih->bh', x_flat, model.ffn.W_in_ih)   # pre-acts\n",
        "        h1 = F.relu(z)                                              # (B, 1024)\n",
        "        h2 = torch.einsum('bh,ho->bo', h1, model.ffn.W_out_ho)      # logits (B, 10)\n",
        "        H1.append(h1.cpu()); H2.append(h2.cpu()); Y.append(yb.cpu())\n",
        "    return torch.cat(H1), torch.cat(H2), torch.cat(Y)\n",
        "\n",
        "# Use the same 1024-wide modelA you trained earlier\n",
        "H1_tr, H2_tr, Y_tr = collect_h1_h2(modelA, dl_train_tx)\n",
        "H1_va, H2_va, Y_va = collect_h1_h2(modelA, dl_val_tx)\n",
        "H1_te, H2_te, Y_te = collect_h1_h2(modelA, dl_test_tx)\n",
        "\n",
        "# Normalize targets h2 (primary MSE reported in this space)\n",
        "mu_h2  = H2_tr.mean(0, keepdim=True)\n",
        "std_h2 = H2_tr.std(0, keepdim=True).clamp_min(1e-6)\n",
        "\n",
        "H2_tr_n = (H2_tr - mu_h2) / std_h2\n",
        "H2_va_n = (H2_va - mu_h2) / std_h2\n",
        "H2_te_n = (H2_te - mu_h2) / std_h2\n",
        "\n",
        "def loader_h1_h2(H1, H2n, bs=256, shuffle=True):\n",
        "    return DataLoader(\n",
        "        TensorDataset(H1, H2n),\n",
        "        batch_size=bs, shuffle=shuffle, num_workers=2, pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "dl_h_tr = loader_h1_h2(H1_tr, H2_tr_n, bs=256, shuffle=True)\n",
        "dl_h_va = loader_h1_h2(H1_va, H2_va_n, bs=256, shuffle=False)\n",
        "dl_h_te = loader_h1_h2(H1_te, H2_te_n, bs=256, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 31 — JumpReLU Transcoder T: h1(1024) → h2(10) with L0 penalty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TXH_LATENTS = 128    # hidden features inside T; adjust if you want sparser/denser\n",
        "TXH_TAU     = 0.1\n",
        "TXH_LAMBDA  = 1e-2   # L0 (soft) weight\n",
        "TXH_LR      = 1e-3\n",
        "TXH_EPOCHS  = 5\n",
        "\n",
        "class TranscoderH1toH2(pl.LightningModule):\n",
        "    def __init__(self, d_in=1024, d_lat=TXH_LATENTS, d_out=10, tau=TXH_TAU,\n",
        "                 init_theta=0.5, lambda_l0=TXH_LAMBDA, lr=TXH_LR):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.enc = nn.Linear(d_in, d_lat, bias=True)\n",
        "        self.theta_h = nn.Parameter(torch.full((d_lat,), init_theta))\n",
        "        self.dec = nn.Linear(d_lat, d_out, bias=True)\n",
        "        self.tau = tau\n",
        "    def forward(self, h1):\n",
        "        u = self.enc(h1)\n",
        "        soft = torch.sigmoid((u - self.theta_h)/self.tau)\n",
        "        hard = (u > self.theta_h).float()\n",
        "        gate = (hard - soft).detach() + soft\n",
        "        f = u * gate\n",
        "        h2_hat_n = self.dec(f)             # predicts normalized h2\n",
        "        return h2_hat_n, f, u, soft, hard\n",
        "    def _step(self, batch):\n",
        "        h1, h2_tgt_n = (t.to(self.device) for t in batch)\n",
        "        h2_hat_n, f, u, soft, hard = self(h1)\n",
        "        recon = F.mse_loss(h2_hat_n, h2_tgt_n, reduction=\"mean\")\n",
        "        l0_soft = soft.sum(dim=1).mean()\n",
        "        loss = recon + self.hparams.lambda_l0 * l0_soft\n",
        "        l0_hard = hard.sum(dim=1).float().mean().detach()\n",
        "        return loss, recon.detach(), l0_soft.detach(), l0_hard\n",
        "    def training_step(self, batch, _):\n",
        "        loss, recon, l0s, l0h = self._step(batch)\n",
        "        self.log_dict({\"train/recon_mse\":recon, \"train/l0_soft\":l0s, \"train/l0_hard\":l0h}, prog_bar=True)\n",
        "        return loss\n",
        "    def validation_step(self, batch, _):\n",
        "        _, recon, l0s, l0h = self._step(batch)\n",
        "        self.log_dict({\"val/recon_mse\":recon, \"val/l0_soft\":l0s, \"val/l0_hard\":l0h}, prog_bar=True)\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "\n",
        "T_h1h2 = TranscoderH1toH2(d_in=H1_tr.shape[1], d_out=H2_tr.shape[1])\n",
        "_tr = _trainer(TXH_EPOCHS)\n",
        "_tr.fit(T_h1h2, dl_h_tr, dl_h_va)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 32 — Metrics: MSE (normalized/raw) + L0 on VAL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def mse_over_pairs(T, H1, H2_raw, mu, std, bs=2048):\n",
        "    T.eval().to(device)\n",
        "    tot_n=tot_r=0.0; cnt_n=cnt_r=0\n",
        "    for i in range(0, H1.size(0), bs):\n",
        "        h1  = H1[i:i+bs].to(device)\n",
        "        h2r = H2_raw[i:i+bs].to(device)\n",
        "        h2n = (h2r - mu.to(device)) / (std.to(device) + 1e-8)\n",
        "        h2hat_n, *_ = T(h1)\n",
        "        # normalized MSE\n",
        "        diff_n = h2hat_n - h2n\n",
        "        tot_n += diff_n.pow(2).sum().item(); cnt_n += diff_n.numel()\n",
        "        # raw MSE (de-normalize)\n",
        "        h2hat_r = h2hat_n * (std.to(device)+1e-8) + mu.to(device)\n",
        "        diff_r = h2hat_r - h2r\n",
        "        tot_r += diff_r.pow(2).sum().item(); cnt_r += diff_r.numel()\n",
        "    return tot_n/cnt_n, tot_r/cnt_r\n",
        "\n",
        "tr_mse_n, tr_mse_r = mse_over_pairs(T_h1h2, H1_tr, H2_tr, mu_h2, std_h2)\n",
        "va_mse_n, va_mse_r = mse_over_pairs(T_h1h2, H1_va, H2_va, mu_h2, std_h2)\n",
        "te_mse_n, te_mse_r = mse_over_pairs(T_h1h2, H1_te, H2_te, mu_h2, std_h2)\n",
        "\n",
        "print(f\"[T(h1)->h2 MSE — normalized] train={tr_mse_n:.6f} | val={va_mse_n:.6f} | test={te_mse_n:.6f}\")\n",
        "print(f\"[T(h1)->h2 MSE — raw]        train={tr_mse_r:.6f}  | val={va_mse_r:.6f}  | test={te_mse_r:.6f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def l0_stats(T, H1, bs=4096):\n",
        "    T.eval().to(device)\n",
        "    acts=[]\n",
        "    for i in range(0, H1.size(0), bs):\n",
        "        h1 = H1[i:i+bs].to(device)\n",
        "        _, _, u, _, _ = T(h1)\n",
        "        acts.append((u > T.theta_h).float().sum(dim=1).cpu())\n",
        "    a = torch.cat(acts, 0)\n",
        "    return float(a.mean()), float(a.median()), float(a.quantile(0.95))\n",
        "\n",
        "l0_mean, l0_med, l0_p95 = l0_stats(T_h1h2, H1_va)\n",
        "print(f\"[T(h1)->h2 L0 — VAL] mean={l0_mean:.2f} | median={l0_med:.2f} | p95={l0_p95:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 33 — Functional check: CE/Acc using h2_hat (raw) as logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This measures how well T replaces the final linear layer of modelA.\n",
        "\n",
        "@torch.no_grad()\n",
        "def ce_acc_from_h1_with_T(H1, Y, T, mu, std, bs=2048):\n",
        "    T.eval().to(device)\n",
        "    ce_sum=0.0; correct=0; seen=0\n",
        "    for i in range(0, H1.size(0), bs):\n",
        "        h1 = H1[i:i+bs].to(device)\n",
        "        y  = Y[i:i+bs].to(device).long()\n",
        "        h2hat_n, *_ = T(h1)\n",
        "        h2hat_r = h2hat_n * (std.to(device)+1e-8) + mu.to(device)  # raw logits\n",
        "        ce_sum += F.cross_entropy(h2hat_r, y, reduction=\"sum\").item()\n",
        "        correct += (h2hat_r.argmax(dim=1) == y).sum().item()\n",
        "        seen += y.numel()\n",
        "    return ce_sum/max(seen,1), correct/max(seen,1)\n",
        "\n",
        "# Baseline logits from modelA (for reference)\n",
        "@torch.no_grad()\n",
        "def ce_acc_modelA_from_x(model, loader):\n",
        "    ce, acc = ce_and_acc_over_img_loader(model, loader, device=device)\n",
        "    return ce, acc\n",
        "\n",
        "ceA_val, accA_val = ce_acc_modelA_from_x(modelA, dl_val_tx)\n",
        "ceA_te,  accA_te  = ce_acc_modelA_from_x(modelA, dl_test_tx)\n",
        "\n",
        "ceT_val, accT_val = ce_acc_from_h1_with_T(H1_va, Y_va, T_h1h2, mu_h2, std_h2)\n",
        "ceT_te,  accT_te  = ce_acc_from_h1_with_T(H1_te, Y_te, T_h1h2, mu_h2, std_h2)\n",
        "\n",
        "print(\"Functional fidelity T(h1)->h2 (same MLP)\")\n",
        "print(f\"VAL  | CE base {ceA_val:.4f} | CE T {ceT_val:.4f} | ΔCE {ceT_val - ceA_val:+.4f} | Acc base {accA_val:.4f} | Acc T {accT_val:.4f} | ΔAcc {accT_val - accA_val:+.4f}\")\n",
        "print(f\"TEST | CE base {ceA_te:.4f}  | CE T {ceT_te:.4f}  | ΔCE {ceT_te - ceA_te:+.4f}  | Acc base {accA_te:.4f}  | Acc T {accT_te:.4f}  | ΔAcc {accT_te - accA_te:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 34: Generate string reversal dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VOCAB = list(\"abcd\")\n",
        "BOS = \"BOS\"\n",
        "PAD = \"<PAD>\"\n",
        "MAX_LEN = 5\n",
        "\n",
        "stoi = {tok: i for i, tok in enumerate([BOS, PAD] + VOCAB)}\n",
        "itos = {i: tok for tok, i in stoi.items()}\n",
        "\n",
        "def generate_example():\n",
        "    seq = np.random.choice(VOCAB, size=np.random.randint(2, MAX_LEN+1)).tolist()\n",
        "    return [BOS] + seq, [BOS] + seq[::-1]\n",
        "\n",
        "def encode(seq, max_len=MAX_LEN+1):\n",
        "    ids = [stoi[t] for t in seq]\n",
        "    while len(ids) < max_len:  # pad\n",
        "        ids.append(stoi[PAD])\n",
        "    return ids\n",
        "\n",
        "N_SAMPLES = 2000\n",
        "data = [generate_example() for _ in range(N_SAMPLES)]\n",
        "inputs, targets = zip(*data)\n",
        "\n",
        "inputs_toks = torch.tensor([encode(seq) for seq in inputs])\n",
        "targets_toks = torch.tensor([encode(seq) for seq in targets])\n",
        "\n",
        "print(\"Input shape:\", inputs_toks.shape, \"Target shape:\", targets_toks.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 35: Tokenize sequences with padding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PAD = \"<PAD>\"\n",
        "stoi = {tok: i for i, tok in enumerate([BOS, PAD] + VOCAB)}\n",
        "itos = {i: tok for tok, i in stoi.items()}\n",
        "\n",
        "def encode(seq, max_len=MAX_LEN+1):  # +1 for BOS\n",
        "    seq_ids = [stoi[t] for t in seq]\n",
        "    # pad with PAD token\n",
        "    while len(seq_ids) < max_len:\n",
        "        seq_ids.append(stoi[PAD])\n",
        "    return seq_ids\n",
        "\n",
        "def decode(seq):\n",
        "    return [itos[i] for i in seq if itos[i] != PAD]\n",
        "\n",
        "inputs_toks = torch.tensor([encode(seq) for seq in inputs])\n",
        "targets_toks = torch.tensor([encode(seq) for seq in targets])\n",
        "\n",
        "print(\"Tokenized input shape:\", inputs_toks.shape)\n",
        "print(\"Tokenized target shape:\", targets_toks.shape)\n",
        "print(\"Example decoded input:\", decode(inputs_toks[0].tolist()))\n",
        "print(\"Example decoded target:\", decode(targets_toks[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 35A: Build reversal Transformer with vocab aligned to tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
        "from tracr.rasp import rasp\n",
        "from tracr.compiler import compiling\n",
        "\n",
        "def build_program(name=\"reverse\"):\n",
        "    if name == \"reverse\":\n",
        "        def make_length():\n",
        "            all_true = rasp.Select(rasp.tokens, rasp.tokens, rasp.Comparison.TRUE)\n",
        "            return rasp.SelectorWidth(all_true)\n",
        "        length = make_length()\n",
        "        opp_index = length - rasp.indices - 1\n",
        "        flip = rasp.Select(rasp.indices, opp_index, rasp.Comparison.EQ)\n",
        "        return rasp.Aggregate(flip, rasp.tokens)\n",
        "    else:\n",
        "        same_index = rasp.Select(rasp.indices, rasp.indices, rasp.Comparison.EQ)\n",
        "        return rasp.Aggregate(same_index, rasp.tokens)\n",
        "\n",
        "program = build_program(\"reverse\")\n",
        "model = compiling.compile_rasp_to_model(\n",
        "    program,\n",
        "    vocab=set(stoi.values()),  # match tokenizer\n",
        "    max_seq_len=MAX_LEN+1,\n",
        "    compiler_bos=BOS,\n",
        ")\n",
        "\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers=model.model_config.num_layers,\n",
        "    d_model=model.params[\"token_embed\"][\"embeddings\"].shape[1],\n",
        "    d_head=model.model_config.key_size,\n",
        "    d_mlp=model.model_config.mlp_hidden_size,\n",
        "    n_heads=model.model_config.num_heads,\n",
        "    n_ctx=model.params[\"pos_embed\"][\"embeddings\"].shape[0],\n",
        "    d_vocab=len(stoi),\n",
        "    d_vocab_out=len(stoi),\n",
        "    act_fn=\"relu\",\n",
        "    attention_dir=\"causal\" if model.model_config.causal else \"bidirectional\",\n",
        "    normalization_type=\"LN\" if model.model_config.layer_norm else None,\n",
        ")\n",
        "tl_model = HookedTransformer(cfg)\n",
        "print(\"tl_model built for reversal\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 36: Run model with cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_, cache = tl_model.run_with_cache(inputs_toks.to(device))\n",
        "print(\"Cache keys:\", list(cache.keys())[:8])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 37: Extract activations (residual stream, last layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layer = tl_model.cfg.n_layers - 1\n",
        "acts = cache[\"resid_post\", layer].detach().cpu().float()\n",
        "acts = acts.reshape(-1, acts.shape[-1])\n",
        "print(\"Acts shape:\", acts.shape)\n",
        "\n",
        "dl_sae = DataLoader(TensorDataset(acts), batch_size=BATCH_SAE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 38: SAE (JumpReLU style)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SAE(nn.Module):\n",
        "    def __init__(self, d_in, d_hidden):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Linear(d_in, d_hidden, bias=False)\n",
        "        self.theta = nn.Parameter(torch.full((d_hidden,), INIT_THETA))\n",
        "        self.dec = nn.Linear(d_hidden, d_in, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = self.enc(x)\n",
        "        # Straight-Through Estimator for JumpReLU\n",
        "        soft = torch.sigmoid((u - self.theta) / TAU)\n",
        "        hard = (u > self.theta).float()\n",
        "        gate = (hard - soft).detach() + soft\n",
        "        h = u * gate\n",
        "        return self.dec(h), h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 39: Train SAE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sae = SAE(d_in=acts.shape[-1], d_hidden=SAE_LATENTS).to(device)\n",
        "opt = torch.optim.Adam(sae.parameters(), lr=LR_SAE_FINAL)\n",
        "for epoch in range(EPOCHS_SAE_FINAL):\n",
        "    for (batch,) in dl_sae:\n",
        "        batch = batch.to(device)\n",
        "        recon, _ = sae(batch)\n",
        "        loss = F.mse_loss(recon, batch)\n",
        "        opt.zero_grad(); loss.backward(); opt.step()\n",
        "    print(f\"Epoch {epoch+1}: loss={loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 40: Inspect SAE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample, = next(iter(dl_sae))\n",
        "recon, h = sae(sample.to(device))\n",
        "print(\"Input shape:\", sample.shape)\n",
        "print(\"Hidden sparsity avg:\", h.sum(-1).float().mean().item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 41: Collect activation database for mechanistic interpretability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This collects sparse latent activations (f_bh) for every MNIST image\n",
        "# along with metadata (image index, label, split) for manual hypothesis generation\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_sae_activations(model, sae, loader, mu_bo, std_bo, split_name=\"train\"):\n",
        "    \"\"\"\n",
        "    Collect SAE latent activations for all images in a dataloader.\n",
        "\n",
        "    Returns:\n",
        "        activations_db: list of dicts with keys:\n",
        "            - 'image_idx': global index within the split\n",
        "            - 'activations': (SAE_LATENTS,) tensor of sparse latent values\n",
        "            - 'label': ground truth digit\n",
        "            - 'split': 'train', 'val', or 'test'\n",
        "            - 'image': (28, 28) original MNIST image\n",
        "    \"\"\"\n",
        "    model.eval().to(device)\n",
        "    sae.eval().to(device)\n",
        "\n",
        "    activations_db = []\n",
        "    global_idx = 0\n",
        "\n",
        "    for x_bchw, y_gt in loader:\n",
        "        x_bchw = x_bchw.to(device)\n",
        "        y_gt = y_gt.to(device)\n",
        "\n",
        "        # Get logits from base model\n",
        "        x_bi = x_bchw.view(x_bchw.size(0), -1)\n",
        "        y_raw_bO = model.ffn(x_bi)\n",
        "\n",
        "        # Normalize logits\n",
        "        y_n_bO = (y_raw_bO - mu_bo.to(device)) / std_bo.to(device)\n",
        "\n",
        "        # Pass through SAE and get sparse latents (f_bh)\n",
        "        y_hat_n_bO, f_bh, u_bh, soft_bh, hard_bh = sae(y_n_bO)\n",
        "\n",
        "        # Store each image's data\n",
        "        for i in range(x_bchw.size(0)):\n",
        "            activations_db.append({\n",
        "                'image_idx': global_idx,\n",
        "                'activations': f_bh[i].cpu(),  # sparse latent activations\n",
        "                'label': y_gt[i].cpu().item(),\n",
        "                'split': split_name,\n",
        "                'image': x_bchw[i, 0].cpu(),  # (28, 28) image\n",
        "            })\n",
        "            global_idx += 1\n",
        "\n",
        "    return activations_db\n",
        "\n",
        "print(\"\\n=== Collecting SAE Activation Database ===\")\n",
        "print(\"This will collect sparse latent activations for all MNIST images...\")\n",
        "\n",
        "# Collect activations for all splits\n",
        "db_train = collect_sae_activations(model, sae_final, dl_train, mu_bo, std_bo, split_name=\"train\")\n",
        "db_val   = collect_sae_activations(model, sae_final, dl_val,   mu_bo, std_bo, split_name=\"val\")\n",
        "db_test  = collect_sae_activations(model, sae_final, dl_test,  mu_bo, std_bo, split_name=\"test\")\n",
        "\n",
        "# Combine all splits\n",
        "activation_database = db_train + db_val + db_test\n",
        "\n",
        "print(f\"Collected activations for {len(activation_database)} images\")\n",
        "print(f\"  Train: {len(db_train)} images\")\n",
        "print(f\"  Val:   {len(db_val)} images\")\n",
        "print(f\"  Test:  {len(db_test)} images\")\n",
        "\n",
        "# Save to disk\n",
        "db_path = os.path.join(OUT_DIR, \"activation_database.pkl\")\n",
        "with open(db_path, \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        'database': activation_database,\n",
        "        'num_latents': SAE_LATENTS,\n",
        "        'metadata': {\n",
        "            'sae_config': {\n",
        "                'latents': SAE_LATENTS,\n",
        "                'tau': TAU,\n",
        "                'init_theta': INIT_THETA,\n",
        "                'target_actives': TARGET_ACTIVES,\n",
        "            },\n",
        "            'normalization': {'mu': mu_bo, 'std': std_bo},\n",
        "        }\n",
        "    }, f)\n",
        "\n",
        "print(f\"\\nSaved activation database to: {db_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 42: Top-k retrieval and neuron statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Functions to analyze which images activate each neuron most strongly\n",
        "\n",
        "def get_top_k_activations(database, neuron_id, k=20):\n",
        "    \"\"\"\n",
        "    Get the top-k images that most strongly activate a specific neuron.\n",
        "\n",
        "    Args:\n",
        "        database: list of dicts from collect_sae_activations\n",
        "        neuron_id: which latent neuron to analyze (0 to SAE_LATENTS-1)\n",
        "        k: how many top images to retrieve\n",
        "\n",
        "    Returns:\n",
        "        list of dicts with keys: image_idx, activation_value, label, split, image\n",
        "    \"\"\"\n",
        "    # Extract activation values for this neuron from all images\n",
        "    results = []\n",
        "    for entry in database:\n",
        "        activation_val = entry['activations'][neuron_id].item()\n",
        "        results.append({\n",
        "            'image_idx': entry['image_idx'],\n",
        "            'activation_value': activation_val,\n",
        "            'label': entry['label'],\n",
        "            'split': entry['split'],\n",
        "            'image': entry['image'],\n",
        "        })\n",
        "\n",
        "    # Sort by activation value (descending) and take top-k\n",
        "    results.sort(key=lambda x: x['activation_value'], reverse=True)\n",
        "    return results[:k]\n",
        "\n",
        "\n",
        "def compute_neuron_statistics(database, neuron_id, activation_threshold=0.0):\n",
        "    \"\"\"\n",
        "    Compute statistics for a specific neuron.\n",
        "\n",
        "    Args:\n",
        "        database: activation database\n",
        "        neuron_id: which neuron to analyze\n",
        "        activation_threshold: minimum activation to count as \"firing\"\n",
        "\n",
        "    Returns:\n",
        "        dict with statistics:\n",
        "            - mean_activation: average activation across all images\n",
        "            - firing_rate: fraction of images where activation > threshold\n",
        "            - label_distribution: dict mapping digit -> count of top activations\n",
        "            - max_activation: maximum activation value seen\n",
        "    \"\"\"\n",
        "    activations = []\n",
        "    firing_count = 0\n",
        "    label_counts = {i: 0 for i in range(10)}\n",
        "\n",
        "    for entry in database:\n",
        "        act_val = entry['activations'][neuron_id].item()\n",
        "        activations.append(act_val)\n",
        "        if act_val > activation_threshold:\n",
        "            firing_count += 1\n",
        "            label_counts[entry['label']] += 1\n",
        "\n",
        "    activations = np.array(activations)\n",
        "\n",
        "    return {\n",
        "        'neuron_id': neuron_id,\n",
        "        'mean_activation': float(activations.mean()),\n",
        "        'max_activation': float(activations.max()),\n",
        "        'firing_rate': firing_count / len(database) if len(database) > 0 else 0.0,\n",
        "        'label_distribution': label_counts,\n",
        "        'num_images': len(database),\n",
        "    }\n",
        "\n",
        "\n",
        "def analyze_all_neurons(database, num_latents=SAE_LATENTS):\n",
        "    \"\"\"\n",
        "    Compute statistics for all neurons in the SAE.\n",
        "\n",
        "    Returns:\n",
        "        list of dicts (one per neuron) with statistics\n",
        "    \"\"\"\n",
        "    stats = []\n",
        "    for neuron_id in range(num_latents):\n",
        "        neuron_stats = compute_neuron_statistics(database, neuron_id)\n",
        "        stats.append(neuron_stats)\n",
        "    return stats\n",
        "\n",
        "\n",
        "# Compute statistics for all neurons\n",
        "print(\"\\n=== Computing Neuron Statistics ===\")\n",
        "neuron_stats = analyze_all_neurons(activation_database, num_latents=SAE_LATENTS)\n",
        "\n",
        "# Save stats\n",
        "stats_path = os.path.join(OUT_DIR, \"neuron_statistics.pkl\")\n",
        "with open(stats_path, \"wb\") as f:\n",
        "    pickle.dump(neuron_stats, f)\n",
        "\n",
        "print(f\"Saved neuron statistics to: {stats_path}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n=== Neuron Statistics Summary ===\")\n",
        "print(f\"{'Neuron':<8} {'Mean Act':<12} {'Max Act':<12} {'Fire Rate':<12} {'Top Label (Count)'}\")\n",
        "print(\"-\" * 70)\n",
        "for stats in neuron_stats[:10]:  # Show first 10 neurons\n",
        "    nid = stats['neuron_id']\n",
        "    mean_act = stats['mean_activation']\n",
        "    max_act = stats['max_activation']\n",
        "    fire_rate = stats['firing_rate']\n",
        "    # Find most common label\n",
        "    top_label = max(stats['label_distribution'].items(), key=lambda x: x[1])\n",
        "    print(f\"{nid:<8} {mean_act:<12.4f} {max_act:<12.4f} {fire_rate:<12.2%} {top_label[0]} ({top_label[1]})\")\n",
        "\n",
        "print(f\"\\n... (showing first 10 of {SAE_LATENTS} neurons)\")\n",
        "\n",
        "\n",
        "# Example: Get top-20 images for neuron 0\n",
        "print(\"\\n=== Example: Top-20 images for Neuron 0 ===\")\n",
        "top_images_n0 = get_top_k_activations(activation_database, neuron_id=0, k=20)\n",
        "print(f\"{'Rank':<6} {'Activation':<12} {'Label':<8} {'Split'}\")\n",
        "print(\"-\" * 40)\n",
        "for rank, img_data in enumerate(top_images_n0[:10], 1):  # Show top 10\n",
        "    print(f\"{rank:<6} {img_data['activation_value']:<12.4f} {img_data['label']:<8} {img_data['split']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 43: Visualization utilities for manual hypothesis generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display top-k activating images for each neuron in a grid\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "def visualize_top_k_for_neuron(database, neuron_id, k=20, save_path=None):\n",
        "    \"\"\"\n",
        "    Create a visualization grid showing the top-k images that activate a neuron.\n",
        "\n",
        "    Args:\n",
        "        database: activation database\n",
        "        neuron_id: which neuron to visualize\n",
        "        k: number of top images to show\n",
        "        save_path: optional path to save the figure\n",
        "    \"\"\"\n",
        "    top_images = get_top_k_activations(database, neuron_id, k=k)\n",
        "\n",
        "    # Compute neuron statistics\n",
        "    stats = compute_neuron_statistics(database, neuron_id)\n",
        "\n",
        "    # Create figure\n",
        "    n_cols = 5\n",
        "    n_rows = (k + n_cols - 1) // n_cols\n",
        "    fig = plt.figure(figsize=(12, 2.5 * n_rows))\n",
        "\n",
        "    # Add title with neuron statistics\n",
        "    top_label = max(stats['label_distribution'].items(), key=lambda x: x[1])\n",
        "    fig.suptitle(\n",
        "        f\"Neuron {neuron_id} | Mean Act: {stats['mean_activation']:.3f} | \"\n",
        "        f\"Fire Rate: {stats['firing_rate']:.1%} | Top Label: {top_label[0]} ({top_label[1]} imgs)\",\n",
        "        fontsize=14, fontweight='bold'\n",
        "    )\n",
        "\n",
        "    # Plot each top image\n",
        "    for idx, img_data in enumerate(top_images):\n",
        "        ax = fig.add_subplot(n_rows, n_cols, idx + 1)\n",
        "        ax.imshow(img_data['image'], cmap='gray')\n",
        "        ax.set_title(\n",
        "            f\"#{idx+1}: {img_data['label']}\\nAct={img_data['activation_value']:.3f}\",\n",
        "            fontsize=9\n",
        "        )\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved visualization to: {save_path}\")\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def visualize_all_neurons(database, num_latents=SAE_LATENTS, k=20, output_dir=None):\n",
        "    \"\"\"\n",
        "    Create visualization grids for all neurons and save them.\n",
        "\n",
        "    Args:\n",
        "        database: activation database\n",
        "        num_latents: number of SAE latents\n",
        "        k: top-k images per neuron\n",
        "        output_dir: directory to save visualizations\n",
        "    \"\"\"\n",
        "    if output_dir is None:\n",
        "        output_dir = os.path.join(OUT_DIR, \"neuron_visualizations\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n=== Generating Visualizations for All {num_latents} Neurons ===\")\n",
        "    for neuron_id in range(num_latents):\n",
        "        save_path = os.path.join(output_dir, f\"neuron_{neuron_id:02d}_top{k}.png\")\n",
        "        fig = visualize_top_k_for_neuron(database, neuron_id, k=k, save_path=save_path)\n",
        "        plt.close(fig)  # Close to avoid memory issues\n",
        "\n",
        "        if (neuron_id + 1) % 5 == 0:\n",
        "            print(f\"  Completed {neuron_id + 1}/{num_latents} neurons\")\n",
        "\n",
        "    print(f\"\\nAll visualizations saved to: {output_dir}\")\n",
        "\n",
        "\n",
        "def create_hypothesis_template(neuron_stats, output_path=None):\n",
        "    \"\"\"\n",
        "    Create a template file for manually recording hypotheses about each neuron.\n",
        "\n",
        "    Args:\n",
        "        neuron_stats: list of neuron statistics dicts\n",
        "        output_path: path to save the template markdown file\n",
        "    \"\"\"\n",
        "    if output_path is None:\n",
        "        output_path = os.path.join(OUT_DIR, \"hypothesis_template.md\")\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        f.write(\"# SAE Neuron Interpretation Hypotheses\\n\\n\")\n",
        "        f.write(f\"Total Neurons: {len(neuron_stats)}\\n\\n\")\n",
        "        f.write(\"---\\n\\n\")\n",
        "\n",
        "        for stats in neuron_stats:\n",
        "            nid = stats['neuron_id']\n",
        "            mean_act = stats['mean_activation']\n",
        "            max_act = stats['max_activation']\n",
        "            fire_rate = stats['firing_rate']\n",
        "            top_label = max(stats['label_distribution'].items(), key=lambda x: x[1])\n",
        "\n",
        "            f.write(f\"## Neuron {nid}\\n\\n\")\n",
        "            f.write(f\"**Statistics:**\\n\")\n",
        "            f.write(f\"- Mean Activation: {mean_act:.4f}\\n\")\n",
        "            f.write(f\"- Max Activation: {max_act:.4f}\\n\")\n",
        "            f.write(f\"- Firing Rate: {fire_rate:.2%}\\n\")\n",
        "            f.write(f\"- Most Common Label: {top_label[0]} ({top_label[1]} images)\\n\")\n",
        "            f.write(f\"- Label Distribution: {dict(stats['label_distribution'])}\\n\\n\")\n",
        "\n",
        "            f.write(f\"**Hypothesis:**\\n\")\n",
        "            f.write(f\"- [ ] Monosemantic (represents one concept)\\n\")\n",
        "            f.write(f\"- [ ] Polysemantic (represents multiple concepts)\\n\")\n",
        "            f.write(f\"- [ ] Dead neuron (rarely/never activates)\\n\\n\")\n",
        "\n",
        "            f.write(f\"**Interpretation:**\\n\")\n",
        "            f.write(f\"> _TODO: Describe what this neuron represents based on top activating images_\\n\\n\")\n",
        "\n",
        "            f.write(f\"**Visual Features:**\\n\")\n",
        "            f.write(f\"- Primary digit(s): \\n\")\n",
        "            f.write(f\"- Key features detected: \\n\")\n",
        "            f.write(f\"- Notes: \\n\\n\")\n",
        "\n",
        "            f.write(\"---\\n\\n\")\n",
        "\n",
        "    print(f\"\\nCreated hypothesis template: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "\n",
        "# Generate visualizations for all neurons\n",
        "visualize_all_neurons(activation_database, num_latents=SAE_LATENTS, k=20)\n",
        "\n",
        "# Create hypothesis template\n",
        "template_path = create_hypothesis_template(neuron_stats)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ACTIVATION DATABASE COLLECTION COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nGenerated files:\")\n",
        "print(f\"  1. {os.path.join(OUT_DIR, 'activation_database.pkl')}\")\n",
        "print(f\"  2. {os.path.join(OUT_DIR, 'neuron_statistics.pkl')}\")\n",
        "print(f\"  3. {os.path.join(OUT_DIR, 'neuron_visualizations/')} (directory with {SAE_LATENTS} images)\")\n",
        "print(f\"  4. {template_path}\")\n",
        "print(\"\\nNext steps for manual interpretation:\")\n",
        "print(\"  1. Review visualizations in neuron_visualizations/\")\n",
        "print(\"  2. Fill out hypothesis_template.md with your observations\")\n",
        "print(\"  3. Identify monosemantic vs polysemantic neurons\")\n",
        "print(\"  4. Document visual features each neuron detects\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
